import asyncio
import logging
from functools import wraps
from typing import AsyncGenerator, Callable, Optional, TypeVar
from contextlib import asynccontextmanager
from sqlalchemy.ext.asyncio import (
    AsyncSession,
    create_async_engine,
    async_sessionmaker,
    AsyncEngine
)
from sqlalchemy.pool import NullPool, AsyncAdaptedQueuePool
from sqlalchemy import event, text, bindparam, inspect
from sqlalchemy.engine import Engine
from sqlalchemy.exc import OperationalError, InterfaceError
import time
from app.config import settings
from app.database.models import Base

logger = logging.getLogger(__name__)

T = TypeVar("T")

# ============================================================================
# PRODUCTION-GRADE CONNECTION POOLING
# ============================================================================

def _is_sqlite_url(url: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ SQLite URL (–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç sqlite:// –∏ sqlite+aiosqlite://)"""
    return url.startswith("sqlite") or ":memory:" in url


DATABASE_URL = settings.get_database_url()
IS_SQLITE = _is_sqlite_url(DATABASE_URL)

if IS_SQLITE:
    poolclass = NullPool
    pool_kwargs = {}
else:
    poolclass = AsyncAdaptedQueuePool
    pool_kwargs = {
        "pool_size": 20,
        "max_overflow": 30,
        "pool_timeout": 30,
        "pool_recycle": 3600,
        "pool_pre_ping": True,
        # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –º–µ—Ä—Ç–≤—ã—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
        "pool_reset_on_return": "rollback",
    }

# ============================================================================
# ENGINE WITH ADVANCED OPTIMIZATIONS
# ============================================================================

# PostgreSQL-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ connect_args
_pg_connect_args = {
    "server_settings": {
        "application_name": "remnawave_bot",
        "jit": "on",
        "statement_timeout": "60000",  # 60 —Å–µ–∫—É–Ω–¥
        "idle_in_transaction_session_timeout": "300000",  # 5 –º–∏–Ω—É—Ç
    },
    "command_timeout": 60,
    "timeout": 10,
}

engine = create_async_engine(
    DATABASE_URL,
    poolclass=poolclass,
    echo=settings.DEBUG,
    future=True,
    # –ö–µ—à —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ä–∞–∑–º–µ—â–µ–Ω–∏–µ)
    query_cache_size=500,
    connect_args=_pg_connect_args if not IS_SQLITE else {},
    execution_options={
        "isolation_level": "READ COMMITTED",
    },
    **pool_kwargs,
)

# ============================================================================
# SESSION FACTORY WITH OPTIMIZATIONS
# ============================================================================

AsyncSessionLocal = async_sessionmaker(
    bind=engine,
    class_=AsyncSession,
    expire_on_commit=False,
    autoflush=False,  # –ö—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    autocommit=False,
)

# ============================================================================
# RETRY LOGIC FOR DATABASE OPERATIONS
# ============================================================================

RETRYABLE_EXCEPTIONS = (OperationalError, InterfaceError, ConnectionRefusedError, OSError)
DEFAULT_RETRY_ATTEMPTS = 3
DEFAULT_RETRY_DELAY = 0.5  # —Å–µ–∫—É–Ω–¥—ã


def with_db_retry(
    attempts: int = DEFAULT_RETRY_ATTEMPTS,
    delay: float = DEFAULT_RETRY_DELAY,
    backoff: float = 2.0,
) -> Callable:
    """
    –î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ retry –ø—Ä–∏ —Å–±–æ—è—Ö –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î.

    Args:
        attempts: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫
        delay: –ù–∞—á–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏ (—Å–µ–∫—É–Ω–¥—ã)
        backoff: –ú–Ω–æ–∂–∏—Ç–µ–ª—å –∑–∞–¥–µ—Ä–∂–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–∏
    """
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(*args, **kwargs):
            last_exception = None
            current_delay = delay

            for attempt in range(1, attempts + 1):
                try:
                    return await func(*args, **kwargs)
                except RETRYABLE_EXCEPTIONS as e:
                    last_exception = e
                    if attempt < attempts:
                        logger.warning(
                            "–û—à–∏–±–∫–∞ –ë–î (–ø–æ–ø—ã—Ç–∫–∞ %d/%d): %s. –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ %.1f —Å–µ–∫...",
                            attempt, attempts, str(e)[:100], current_delay
                        )
                        await asyncio.sleep(current_delay)
                        current_delay *= backoff
                    else:
                        logger.error(
                            "–û—à–∏–±–∫–∞ –ë–î: –≤—Å–µ %d –ø–æ–ø—ã—Ç–æ–∫ –∏—Å—á–µ—Ä–ø–∞–Ω—ã. –ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: %s",
                            attempts, str(e)
                        )

            raise last_exception
        return wrapper
    return decorator


async def execute_with_retry(
    session: AsyncSession,
    statement,
    attempts: int = DEFAULT_RETRY_ATTEMPTS,
):
    """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ SQL —Å retry –ª–æ–≥–∏–∫–æ–π."""
    last_exception = None
    delay = DEFAULT_RETRY_DELAY

    for attempt in range(1, attempts + 1):
        try:
            return await session.execute(statement)
        except RETRYABLE_EXCEPTIONS as e:
            last_exception = e
            if attempt < attempts:
                logger.warning(
                    "SQL retry (–ø–æ–ø—ã—Ç–∫–∞ %d/%d): %s",
                    attempt, attempts, str(e)[:100]
                )
                await asyncio.sleep(delay)
                delay *= 2

    raise last_exception


# ============================================================================
# QUERY PERFORMANCE MONITORING
# ============================================================================

if settings.DEBUG:
    @event.listens_for(Engine, "before_cursor_execute")
    def before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
        conn.info.setdefault("query_start_time", []).append(time.time())
        logger.debug(f"üîç Executing query: {statement[:100]}...")

    @event.listens_for(Engine, "after_cursor_execute")
    def after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
        total = time.time() - conn.info["query_start_time"].pop(-1)
        if total > 0.1:  # –õ–æ–≥–∏—Ä—É–µ–º –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã > 100ms
            logger.warning(f"üêå Slow query ({total:.3f}s): {statement[:100]}...")
        else:
            logger.debug(f"‚ö° Query executed in {total:.3f}s")

# ============================================================================
# ADVANCED SESSION MANAGER WITH READ REPLICAS
# ============================================================================

HEALTH_CHECK_TIMEOUT = 5.0  # —Å–µ–∫—É–Ω–¥—ã


def _validate_database_url(url: Optional[str]) -> Optional[str]:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è URL –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö."""
    if not url:
        return None
    url = url.strip()
    if not url or url.isspace():
        return None
    # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
    if not ("://" in url or url.startswith("sqlite")):
        logger.warning("–ù–µ–≤–∞–ª–∏–¥–Ω—ã–π DATABASE_URL: %s", url[:20])
        return None
    return url


class DatabaseManager:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –ë–î —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–µ–ø–ª–∏–∫ –∏ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è"""

    def __init__(self):
        self.engine = engine
        self.read_replica_engine: Optional[AsyncEngine] = None
        self._read_replica_session_factory: Optional[async_sessionmaker] = None

        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ —Å–æ–∑–¥–∞–Ω–∏–µ read replica engine
        replica_url = _validate_database_url(
            getattr(settings, 'DATABASE_READ_REPLICA_URL', None)
        )
        if replica_url:
            try:
                self.read_replica_engine = create_async_engine(
                    replica_url,
                    poolclass=poolclass,
                    pool_size=30,  # –ë–æ–ª—å—à–µ –¥–ª—è read –æ–ø–µ—Ä–∞—Ü–∏–π
                    max_overflow=50,
                    pool_pre_ping=True,
                    pool_recycle=3600,
                    echo=False,
                )
                # –°–æ–∑–¥–∞—ë–º sessionmaker –æ–¥–∏–Ω —Ä–∞–∑ (–Ω–µ –ø—Ä–∏ –∫–∞–∂–¥–æ–º –≤—ã–∑–æ–≤–µ)
                self._read_replica_session_factory = async_sessionmaker(
                    bind=self.read_replica_engine,
                    class_=AsyncSession,
                    expire_on_commit=False,
                    autoflush=False,
                )
                logger.info("Read replica –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞: %s", replica_url[:30] + "...")
            except Exception as e:
                logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å read replica: %s", e)
                self.read_replica_engine = None

    @asynccontextmanager
    async def session(self, read_only: bool = False):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Å–µ—Å—Å–∏–µ–π –ë–î."""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥—Å–æ–∑–¥–∞–Ω–Ω—ã–π sessionmaker –≤–º–µ—Å—Ç–æ —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ
        if read_only and self._read_replica_session_factory:
            session_factory = self._read_replica_session_factory
        else:
            session_factory = AsyncSessionLocal

        async with session_factory() as session:
            try:
                yield session
                if not read_only:
                    await session.commit()
            except Exception:
                await session.rollback()
                raise

    async def health_check(self, timeout: float = HEALTH_CHECK_TIMEOUT) -> dict:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è –ë–î —Å —Ç–∞–π–º–∞—É—Ç–æ–º.

        Args:
            timeout: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è (—Å–µ–∫—É–Ω–¥—ã)
        """
        pool = self.engine.pool
        status = "unhealthy"
        latency = None

        try:
            async with asyncio.timeout(timeout):
                async with AsyncSessionLocal() as session:
                    start = time.time()
                    await session.execute(text("SELECT 1"))
                    latency = (time.time() - start) * 1000
            status = "healthy"
        except asyncio.TimeoutError:
            logger.error("Health check —Ç–∞–π–º–∞—É—Ç (%s —Å–µ–∫)", timeout)
            status = "timeout"
        except Exception as e:
            logger.error("Database health check failed: %s", e)
            status = "unhealthy"

        return {
            "status": status,
            "latency_ms": round(latency, 2) if latency else None,
            "pool": _collect_health_pool_metrics(pool),
        }

    async def health_check_replica(self, timeout: float = HEALTH_CHECK_TIMEOUT) -> Optional[dict]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è read replica."""
        if not self.read_replica_engine:
            return None

        pool = self.read_replica_engine.pool
        status = "unhealthy"
        latency = None

        try:
            async with asyncio.timeout(timeout):
                async with self._read_replica_session_factory() as session:
                    start = time.time()
                    await session.execute(text("SELECT 1"))
                    latency = (time.time() - start) * 1000
            status = "healthy"
        except asyncio.TimeoutError:
            status = "timeout"
        except Exception as e:
            logger.error("Read replica health check failed: %s", e)

        return {
            "status": status,
            "latency_ms": round(latency, 2) if latency else None,
            "pool": _collect_health_pool_metrics(pool),
        }


db_manager = DatabaseManager()

# ============================================================================
# SESSION DEPENDENCY FOR FASTAPI/AIOGRAM
# ============================================================================

async def get_db() -> AsyncGenerator[AsyncSession, None]:
    """–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è dependency –¥–ª—è FastAPI"""
    async with AsyncSessionLocal() as session:
        try:
            yield session
            await session.commit()
        except Exception:
            await session.rollback()
            raise

async def get_db_read_only() -> AsyncGenerator[AsyncSession, None]:
    """Read-only dependency –¥–ª—è —Ç—è–∂–µ–ª—ã—Ö SELECT –∑–∞–ø—Ä–æ—Å–æ–≤"""
    async with db_manager.session(read_only=True) as session:
        yield session

# ============================================================================
# BATCH OPERATIONS FOR PERFORMANCE
# ============================================================================

class BatchOperations:
    """–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –º–∞—Å—Å–æ–≤—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π"""
    
    @staticmethod
    async def bulk_insert(session: AsyncSession, model, data: list[dict], chunk_size: int = 1000):
        """–ú–∞—Å—Å–æ–≤–∞—è –≤—Å—Ç–∞–≤–∫–∞ —Å —á–∞–Ω–∫–∞–º–∏"""
        for i in range(0, len(data), chunk_size):
            chunk = data[i:i + chunk_size]
            session.add_all([model(**item) for item in chunk])
            await session.flush()
        await session.commit()
    
    @staticmethod
    async def bulk_update(session: AsyncSession, model, data: list[dict], chunk_size: int = 1000):
        """–ú–∞—Å—Å–æ–≤–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å —á–∞–Ω–∫–∞–º–∏"""
        if not data:
            return

        primary_keys = [column.name for column in model.__table__.primary_key.columns]
        if not primary_keys:
            raise ValueError("Model must have a primary key for bulk_update")

        updatable_columns = [
            column.name
            for column in model.__table__.columns
            if column.name not in primary_keys
        ]

        if not updatable_columns:
            raise ValueError("No columns available for update in bulk_update")

        stmt = (
            model.__table__.update()
            .where(
                *[
                    getattr(model.__table__.c, pk) == bindparam(pk)
                    for pk in primary_keys
                ]
            )
            .values(
                **{
                    column: bindparam(column, required=False)
                    for column in updatable_columns
                }
            )
        )

        for i in range(0, len(data), chunk_size):
            chunk = data[i:i + chunk_size]
            filtered_chunk = []
            for item in chunk:
                missing_keys = [pk for pk in primary_keys if pk not in item]
                if missing_keys:
                    raise ValueError(
                        f"Missing primary key values {missing_keys} for bulk_update"
                    )

                filtered_item = {
                    key: value
                    for key, value in item.items()
                    if key in primary_keys or key in updatable_columns
                }
                filtered_chunk.append(filtered_item)

            await session.execute(stmt, filtered_chunk)
        await session.commit()

batch_ops = BatchOperations()

# ============================================================================
# INITIALIZATION AND CLEANUP
# ============================================================================

async def init_db():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
    logger.info("–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")

    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)

    if not IS_SQLITE:
        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏...")

        async with engine.begin() as conn:
            indexes = [
                ("users", "CREATE INDEX IF NOT EXISTS idx_users_telegram_id ON users(telegram_id)"),
                (
                    "subscriptions",
                    "CREATE INDEX IF NOT EXISTS idx_subscriptions_user_id ON subscriptions(user_id)",
                ),
                (
                    "subscriptions",
                    "CREATE INDEX IF NOT EXISTS idx_subscriptions_status ON subscriptions(status) WHERE status = 'active'",
                ),
                (
                    "payments",
                    "CREATE INDEX IF NOT EXISTS idx_payments_created_at ON payments(created_at DESC)",
                ),
            ]

            for table_name, index_sql in indexes:
                table_exists = await conn.run_sync(lambda sync_conn: inspect(sync_conn).has_table(table_name))

                if not table_exists:
                    logger.debug(
                        "–ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ %s: —Ç–∞–±–ª–∏—Ü–∞ %s –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç",
                        index_sql,
                        table_name,
                    )
                    continue

                try:
                    await conn.execute(text(index_sql))
                except Exception as e:
                    logger.debug("Index creation skipped for %s: %s", table_name, e)

    logger.info("–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")

    health = await db_manager.health_check()
    logger.info("Database health: %s", health)


async def close_db():
    """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–∞–∫—Ä—ã—Ç–∏–µ –≤—Å–µ—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π"""
    logger.info("–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π —Å –ë–î...")

    await engine.dispose()

    if db_manager.read_replica_engine:
        await db_manager.read_replica_engine.dispose()

    logger.info("–í—Å–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –∑–∞–∫—Ä—ã—Ç—ã")

# ============================================================================
# CONNECTION POOL METRICS (–¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞)
# ============================================================================

def _pool_counters(pool):
    """Return basic pool counters or ``None`` when unsupported."""

    required_methods = ("size", "checkedin", "checkedout", "overflow")

    for method_name in required_methods:
        method = getattr(pool, method_name, None)
        if method is None or not callable(method):
            return None

    size = pool.size()
    checked_in = pool.checkedin()
    checked_out = pool.checkedout()
    overflow = pool.overflow()

    total_connections = size + overflow

    return {
        "size": size,
        "checked_in": checked_in,
        "checked_out": checked_out,
        "overflow": overflow,
        "total_connections": total_connections,
        "utilization_percent": (checked_out / total_connections * 100) if total_connections else 0.0,
    }


def _collect_health_pool_metrics(pool) -> dict:
    counters = _pool_counters(pool)

    if counters is None:
        return {
            "metrics_available": False,
            "size": 0,
            "checked_in": 0,
            "checked_out": 0,
            "overflow": 0,
            "total_connections": 0,
            "utilization": "0.0%",
        }

    return {
        "metrics_available": True,
        "size": counters["size"],
        "checked_in": counters["checked_in"],
        "checked_out": counters["checked_out"],
        "overflow": counters["overflow"],
        "total_connections": counters["total_connections"],
        "utilization": f"{counters['utilization_percent']:.1f}%",
    }


async def get_pool_metrics() -> dict:
    """–î–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø—É–ª–∞ –¥–ª—è Prometheus/Grafana"""
    pool = engine.pool

    counters = _pool_counters(pool)

    if counters is None:
        return {
            "metrics_available": False,
            "pool_size": 0,
            "checked_in_connections": 0,
            "checked_out_connections": 0,
            "overflow_connections": 0,
            "total_connections": 0,
            "max_possible_connections": 0,
            "pool_utilization_percent": 0.0,
        }

    return {
        "metrics_available": True,
        "pool_size": counters["size"],
        "checked_in_connections": counters["checked_in"],
        "checked_out_connections": counters["checked_out"],
        "overflow_connections": counters["overflow"],
        "total_connections": counters["total_connections"],
        "max_possible_connections": counters["total_connections"] + (getattr(pool, "_max_overflow", 0) or 0),
        "pool_utilization_percent": round(counters["utilization_percent"], 2),
    }
